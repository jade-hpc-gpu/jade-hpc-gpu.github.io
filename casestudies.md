---
title: JADE Case Studies and Success Stories
layout: page
permalink: "/casestudies/"
---


--------------------------
## Oxford University
[**Mapping the old with the new: handheld 3D scanning of Oxford's Sheldonian Theatre**](https://eng.ox.ac.uk/news/handheld-3d-scanning-of-oxfords-sheldonian-theatre/)<br>
[**Oxford quadruped robot research-tested on a live Chevron facility**](https://eng.ox.ac.uk/news/oxford-quadruped-robot-research-tested-on-a-live-chevron-facility/)<br>

## Newcastle University 
**Dermatologically Inspired Deep Face Analytics** 
<br>
Introduction 

The ability to detect face ageing which is localised to specific regions of the face has shown to be useful to both the medical and research communities. In medical practice it could be used to inform the diagnosis of conditions, and subsequently the speed and accuracy of screening. However, large-scale facial screening by human experts is laborious and requires specialised expertise. Hence, there is great value in the development of automated methods for feature extraction and decision making from facial Imagery in a health and well-being context 
<br>

Explaining the Science <br>
The main challenge in training any machine learning problem is data. In order to train a deep neural network to predict something accurately, there must be a large number of known data points. When approaching the field of ageing, there is no dataset in existence labelling the rate at which different face regions have aged. 

Our approach to this lack of data is to build on the work of others who have developed models for global face ageing and semantic face segmentation. Semantic face segmentation is a field which splits face regions at a pixel level. We intend to learn from both existing datasets of segmented faces and datasets containing global face ages. The combination of these two types of data provide a framework to develop novel new methods for grading the ageing of faces at a feature level. 

Aside from grading the uneven rate of ageing across the human face, we also access a novel dataset of elderly individuals who have died since the images were taken. We use these faces in combination with the number of years they lived following the photograph, to attempt to extract features relevant to their mortality.  <br>

Project aims <br>
The aims and objectives of this project are interdisciplinary at their core. Ranging from novel contributions in data annotation, to neural network architecture; to dermatological analyses. We intend to explore the active research area of ageing and vitality, covering gaps in existing research and propose a new deep learning model to analyse the heterogeneous ageing of human faces. While there is existing work detailing both human and computational methods for grading face ageing, little research has been done to apply deep learning to this problem. We aim to train convolutional neural networks to extract features related to age, health and ultimately mortality. <br>

Applications <br>
Our work is highly applicable to several practical and research areas, with many more possible applications materialising as the wealth of academic data grows. 

The ability to accurately extract age and health related features from face images allows Dermatological research to be carried on sample sizes which are several orders of magnitude larger than without. Presently clinical trials for dermatological treatment are run in labs, with expensive photography equipment and highly skilled reviewers needed to evaluate outcomes. Given an effectively trained neural network, these analyses can be carried out automatically using selfie images from mobile phones; allowing participants to be located anywhere globally. The benefits of remote face skin analysis also extend to every-day practice, where pre-appointment screening can be carried out to filter patients who may not need to see a doctor immediately. 

Given a mortality feature extractor with a good correlation to the endpoint of death, it is possible to implement screening systems in high risk locations such as GP surgeries and Care Homes. These systems could be designed to flag individuals with a higher relative mortality risk for their age, allowing them to receive care in a timely fashion. 

**Estimating individual pig liveweight trajectories from group-level weight measurements**

Introduction <br>
Recent years have seen an increase in the use of advanced data collection systems to improve management in pig farms, however, the cost of implementing such systems remains a barrier to many. Some of the most useful data for farm management are on an individual pig level, which requires investment of labour and expense of systems such as RFID tagging. Machine learning approaches can help to alleviate this issue for instance by estimating individual growth trajectories from unknown weight measurements without requiring expensive RFID systems. 

Explaining the science <br>
Machine learning methods can be used to estimate individual-level pig growth trajectories as a cheaper alternative to using RFID tag systems. However, the use of traditional machine learning methods in this task has several limitations, namely that they cannot incorporate all data on a group-level time series simultaneously. This limitation could have a potentially profound impact on their predictive accuracy and thus also their applicability. 

Project aims <br>
This project aims to develop more accurate methods for the prediction of individual-level growth trajectories. To achieve this, attention methods capable of integrating all contextual data within group-level time series will be developed, and comparisons between these methods and existing machine learning methods will be made. 

This project will also involve the exploration of attention weights produced by the attention methods to allow for the interpretation of resulting predictions. In addition, this project seeks to develop additional metrics for evaluating the accuracy of predicted growth trajectories, and to improve the ability of methods to generalize to pigs of different breeds than those that they were trained on. 

Applications <br>
The methods developed during this project have the potential to be used as a significantly cheaper alternative to RFID tagging systems for the tracking of individual-level pig weights when combined with either weighting platforms or camera-based pig weight estimation systems. This work could also be used as a foundation for the development of methods that can be applied to other livestock.

## Loughborough University

Yang Zhou 

As a PhD student from Department of Computer Science at Loughborough University, I have been using the JADE-2 to train an anomaly detection model for underwater unknown detection. In this case, apart from the normal layers like convolutional layers, activation functions etc., I implemented a new layer named Mask Fully Connected (MFC) layer, which was successfully run on the JADE-2. Four datasets were used in this experiment: CIFAR-10 dataset with 60,000 images, a simulation dataset with around 100 images, an underwater statue dataset around 4,000 images and a public underwater dataset SUIM around 5,000 images. Thus, I used JADE-2 to train different models on these datasets respectively and train the comparison methods such as GAN, VAE and LSA models as well. With JADE-2, the training process can be accelerated, and the model can be easily downloaded to my workstation for further validation locally. 

Lei Jiang  

I am a PhD student from the computer science department, Loughborough University. In my case, I mainly use Jade2 time to train/test models in novel view synthesis based on depth or neural radiance fields with the help of multi-gpus. The trained model can recover partial or completed 3D information from input images, allowing the further view transformation and neural rendering for new views. Two datasets including ShapeNet and KITTI dataset are used to validate the effectiveness of my method on JADE2. 

Jinze Huo 

My research interest lies in skeleton-based action recognition and my approach uses GCN to analyse skeleton data. I have to utilise Jade2 because the skeleton dataset is too big. The first paper has been published as below, while the second and third projects are in progress. 


Ye Wei 

The reason for me to use the JADE2 HPC is to train a traffic forecasting model based on Graph Neural Networks (GNNs). Traffic forecasting aims to predict future traffic conditions (e.g., congestion class or speed) in road networks based on historical observations (e.g., sensor recording). This task is an indispensable part of Intelligent Traffic System (ITS) and it has a wide range of applications from route planning and travel time estimation, to supply chain management. Since the network structure of urban cities is large and complex, GNN models are very slow to train on ordinary graphics cards such as RTX 2080 Ti. With the help of JADE2, the training process can be accelerated, and this can greatly enhance the efficiency of my research. 

Yixiao Zhang 

I developed a number of variants of Transformer neural network to recognize audio events. The training of Transformer costs a lot of time on local deep learning machines, so I used JADE2 to speed up my experiments. JADE2 has a strong computational power which significantly reduced my experiment time. 

Siyuan Deng 

I have been utilising the JADE-2 to train a re-identification model for visible-infrared person as a PhD student at Loughborough University's Department of Computer Science. In this instance, I employ JADE-2 to establish a baseline for a recent study that employs three modalities and contrastive learning to create an aggregate memory-based deep metric learning framework. The benchmark VIReID dataset SYSU-MM01 that was used has 491 identities that were gathered from two near-infrared cameras and four visible cameras. The training set contain 395 identities with 22,258 RGB and 11,909 infrared images. With JADE-2, the model can be trained to the baseline and quickly downloaded to the local workspace. The work submission and environment setup, however, are a little challenging. 

Jiangtao Wang 

JADE-2 is used to train and validate several computer vision deep networks in my PhD study. We used to employ single or dual NVIDIA graphic cards to develop, train and improve the neural networks for our underwater robot's exploration. This including the image segmentation, image enhancement, depth estimation and other robotic tasks. 

However once dealing with high quality images (e.g. 2k resolution) we used to wait for quiet longer time to complete network training and much longer time to improve its performance. As long as we moved our work to JADE-2, we use typical configurations such as 8 GPUs to train network in parallel to significantly save time from network training. 

Wenjuan Zhou 

Title: Visual focus of attention detection in an unconstrained environment 

The visual focus of attention (VFOA) is this kind of human behaviour which is determined by eye gaze and head pose dynamics. We need to estimate the Gaze to determine where the visual focus of attention is. Gaze estimation can be applied to many scenarios, such as driving, and human-robot interaction. In addition, the detection of abnormal gaze is also widely used in the medical field. For example, in the standard of early screening and treatment of autism spectrum disorder (ASD), reduced eye contact is considered an important indicator. There is a robot-assisted system that can help with the treatment in the early stage of ASD. In this system, the robot needs to judge the child’s gaze information to give the corresponding response. If the robot wants to interact with humans, the robot needs to determine whether the gaze attention of the person is concentrated on the robot or not, this will help the robot to catch the timing of interacting with the human. 

The visual focus of attention detection as an important topic under the Human-Robot interaction scenes faced the huge challenge of low detection accuracy in real-world applications compared with laboratory scenes. Since most of the images obtained through the robot camera have no restrictions on the light source, posture, and scene. Recently, with the development of deep learning technology, the performance of visual focus of attention detection-based deep learning models has been significantly improved. We aim to build deep learning models that enable the robot to detect the visual focus of attention in an unconstrained environment in real-time. For achieving the aim, we need a larger dataset to train the deeper model we built, this needs a huge number of computational resources, and we would like to use Jade2 HPC to speed up our experiments and finish our study quickly. 

Up till now, we have already configured the environment on Jade2 to train models. For the next step, we will train the models we build, and enlarge the annotated training dataset to achieve better performance in our study. 

## Kings College London
**JADE User Project Summaries** 

"Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance for many medical segmentation tasks when trained in a fully-supervised manner, i.e. by using pixel-wise annotations. In practice, CNNs require large annotated datasets to be able to generalise to different acquisition protocols and to cover variability in the data (e.g. size and location of a pathology). Given the time and expertise required to carefully annotate medical data, the data-labelling process is a key bottleneck for the development of automatic image medical segmentation tools. To address this issue, we would like to investigate techniques that exploit weak annotations (e.g. scribbles, bounding boxes, extreme points) instead of time-consuming pixel-wise annotations." 

The aim of this project is to develop computational tools that account for error and bias caused by MRI scanner upgrade during longitudinal studies of neurodegenerative diseases such as Alzheimer’s disease or fronto-temporal dementia. 

This project aims to build truly scalable algorithms to advance the state of the art in incremental learning and adapt these algorithms to equip robots with lifelong perception and interaction capabilities. Continual learning aims to design systems that can keep learning new knowledge while maintaining the performance for the previously learned tasks. However, so far approaches considering a robotics context is scarce. We focus on developing novel continual learning algorithms by taking into account the limitations of the robotics platforms (e.g., space and computational power). We developed a novel algorithm, Information Back Discrete Representation Replay, that achieved SOTA performance in class-incremental setting. We developed a novel method, Code Feature Mapping Network, to address the efficient continual robot learning. We are currently working on a novel task-incremental learning method, which achieves the SOTA results in terms of accuracy and memory cost. 

Development and refinement of AI-enabled virtual assistant technology for real-time ultrasound scanning. We're developing Deep learning-based computer vision, biometry, diagnostic support and video summarization tools for clinical use in the FASP UK 20-week foetal ultrasound anomaly scan. 

The goal of this research project is to improve the classical reinforcement learning framework to enable practical and scalable deployments of autonomous agents in the real world. In particular, we are designing novel machine learning algorithms and practices to improve the sample and computational efficiency of the field and remove the reliance on task-specific, hand-engineered practices and objective functions. 

Domain adaptation in crowd counting has gained much success. However, in real-world applications, it's laborious and undesirable to collect and annotate new crowd data when encountering a new scene. When the target domain is inaccessible, domain generalization methods are devised to learn a model that generalizes well to unseen domains. In this research, we plan to study the problem of multi-source domain generalization in crowd counting. Our aim is to decompose the model into feature extractor and density map regressor. We will utilize meta learning with a gradient or entropy-based method to train a domain-agnostic feature extractor. And then learn multiple domain-specific regressors via domain-specific batch normalization or training data from specific domains. 

Combining MD data and HDX-MS data, we first characterized the distinct features of XylE upon substrate xylose and inhibitor glucose binding. Currently, we are interested in investigating the protein dynamic difference of XylE binding to different GLUT-1 inhibitors in a single conformation. 

We aim to stabilise a robot for vitreoretinal surgery in lateral dimensions by detecting the tools and motion of the retina. 

My project aims at improving automated medical image registration for abdominal scans. To achieve this goal, I employ deep learning models. 

The objective of this project is to develop a decision-making tool that identifies abnormalities on magnetic resonance imaging (MRI) brain scans using deep learning. A clinically validated decision-making tool that identifies abnormalities on brain MRI scans does not exist. Immediate triage of a brain MRI into normal or abnormal, which will allow radiology departments to prioritise their limited resources into first reporting abnormal scans. Abnormal scan reports will be reported more quickly than is currently the situation, which will expedite early intervention from the referring clinical team. Specific examples relate to cancer or infective processes. The project will use a retrospective dataset of »100,000, minimally processed, MRI scans from three different sites, which have been labelled by a combination of consultant neuroradiologists and a validated NLP algorithm (ALARM). A classification model is to be built and validated using this data to determine abnormal and normal scans. This model would have to be generalisable and would need to be validated within this sample and externally, on a prospective dataset of »1000 scans from these study sites and from other secondary and tertiary NHS centres across the UK. Further characterisation of the abnormalities detected are a secondary outcome. Exploring unsupervised and semi-supervised methodologies, starting with the use of autoencoders and exploration of the lower dimensional space generated is the first step towards this. 

The aim of this PhD is to model development of glioblastoma in MRIs acquired in routine clinical practice using deep learning. 

This project investigates how 3D structure may be obtained from various kinds of microscopy images through automated processes, and the impacts this might have for further biology research. We address this challenge by focusing in two key areas: by enhancing high-throughput C.elegans experiments with artificial intelligence and machine learning and reconstructing the CEP152 protein complex from STORM images. The C.elegans case has two key limitations: first, the neurons are identified by hand, which is very labour intensive; and second, the need for highly standardised images means many images are discarded. The first issue offers an opportunity to automate cell identification, either by applying supervised machine learning (e.g. random forest classifier) to tens of thousands of annotated image stacks; or by using deep learning so that the algorithm can select the parameters to optimise. The second issue means that there are many unannotated image stacks that can be analysed by unsupervised deep learning and cross-validated against the annotated subset to check performance. Existing evidence supports the hypothesis that the structure of the CEP152 complex is a torus. A large number of images of this complex already exist. We aim to investigate this data using deep-learning techniques to prove this hypothesis further. Several challenges exist, such as noise from the fluoresence microscopy technique, which must be overcome. In both of these cases, structure and orientation are intertwined, providing a basis for identifying areas of interest within a traditional image. We hypothesis that deep-learning and artificial intelligence can detect structure and orientation from a set of images, leading to identification of objects within a single image, at high speed. 

This project aims to make progress in three related areas of deep learning. Firstly, deep learning models must be able to fail reliably and loudly when generating incorrect outputs. Secondly, deep learning models should be able to take advantage of large amounts of unlabeled data, even when deployed on tasks such as segmentation that typically require some human generated labels for the model to learn the task, so that they can generalize effectively. Thirdly, models in clinical pipelines should be able to learn continuously once deployed, in order to take advantage of the evolving ground truth present in electronic health records, and be able to learn from interaction with a human expert correcting the model’s performance. The purpose of this work is to find compatible and complementary approaches that augment baseline networks with the above three properties, with the ultimate goal of enabling new clinical applications for deep learning models. 

In the research, we aim at designing and improving the COVID-19 automatic classification system based on deep learning techniques to assist clinicians in screening of infected patients. The existing viral diagnosis requires long waiting time to obtain the test results that are not accurate enough. Motivated by this fact, we plan to develop an automatic Chest X-ray and Computed Tomography classification system that can provide accurate and sensitive results, accelerate the testing process, alleviate the clinicians’ workload, and control the further spread of the coronavirus. 

A study of nonrigid registration methods for tracking local features of the left atrium in retrospective gated CT images. This includes deep learning-based approaches, for which I require GPU access and usage. 

icovid project looks at develop AI solution for covid lesion detection. We are currently looking into developing interactive segmentation methods for covid lesion annotation that will be used to improve quality and quantity of labelled CT volumes 

Modelling uncertainty in segmentation predictions 

I am trying to test diverse supervised and unsupervised algorithms on drug databases to find the algorithm with the best performances for this type of data. The ML generated model can be used to find new candidates against the certain validated target protein. Running molecular docking followed by Molecular Dynamics (MD) simulations can validate the model and results in terms of the binding free energy of the diverse ligand-target complexes. 

Predicting autism spectrum phenotypes from neonatal brain connectivity: The goal of this project is to improve our understanding of brain development and outcome in babies at risk of autism spectrum disorders (ASD) by: <br>
1.Training machine learning algorithms in adult populations to classify ASD from controls <br>
2.Transferring techniques to state-of-the-art neonatal acquisitions <br>
3.Interpreting algorithms to find altered connectivity patterns associated with outcome <br>
More information [here.](https://gtr.ukri.org/projects?ref=studentship-2269804) 

In this project we aim to provide better understanding and improve the current methods in diagnosis and prognosis of Tuberculous meningitis, leveraging state-of-the art method and MR imaging data. 

Visual tracking is a fundamental task in computer vision with numerous applications in surveillance, self-driving vehicles and UAV-based monitoring. It is the task of locating the same moving object in each frame of a video sequence, given only the initial appearance of the target object. Most modern trackers treat visual tracking as a classification problem. By learning an appearance model of the target from the initial frame, the trackers distinguish the target from the background and other objects by a cross-correlation operation to predict its location in the following frames. Although they ignore other potentially useful sources of information, such as previous motion trajectory, these Tracking-by-Detection methods achieve impressive performance. However, they can fail when the appearance model misidentifies a similar-looking object (a ``distractor'') as the target. This project focuses on this tracking drift problem caused by distractors, and adopts three novel solutions: (1) making the appearance model more sensitive to shape cues in offline training, (2) using the appearance of distractors to prevent other objects being mis-identified as the target, and (3) using the historical locations of the target to predict its future position during online inference. 

Tiny object detection is a heated discussed subfield of object detection. Current methods such as SM, fusion factor, SSPNet make effective modifications mainly on how to utilize features of different scales and extend training set. But I believe the commonly used networks have suboptimal architectures for tiny object detection. I am exploring a thought guiding the design of backbones. 

improve adversarial robustness and interpretability of deep neural networks, research the connnetion between these two properties. 

The goal of this project is to evaluate the use of novel contrastive learning methods for improving the performance of transformers for the task of semantic segmentation in medical and natural image datasets. 

We are trying to find different ways of understanding haptic and visual data using cross modality analysis, in order to improve the quality and reduce the amount of data exchange during telesurgical procedures. We are focusing on the use of attention driven convolutional networks and vision transformer for this type of task, to isolate the the parts on the image were the actual effect of the forces is clearer. 

During the second half of pregnancy, the human foetus undergoes exuberant growth, with both microscopic and macroscopic changes happening rapidly. In consequence, the MRI relaxation times (T1 and T2), which are key tissue properties, and other quantitative measures change substantially. MRI relaxation times can be used to characterise development and assess the brain in health and disease. However, there are no established methods for doing this in the foetus in utero. Initial steps have been taken to achieve other quantitative measures, but these remain in an early stage of development and are currently very demanding in acquisition and computation time. The aim of the project is to develop reliable motion tolerant methods for quantitatively mapping key parameters in the moving foetus with the help of novel deep learning solutions and deploy these to conduct systematic quantitative studies over gestational age and provide normative benchmark data. 

This project aims at overcoming the limitations of learning surgical instrument segmentation models with small curated data and further improving the current state of arts to a higher level. Firstly, a visual language model will be trained from more than 100K surgical (image, text) pairs from the internet. Secondly, transfer learning will be used by finetuning the pre-trained visual language model on small curated datasets to improve segmentation performances on fixed tasks. Further, semi-supervised learning will be explored to increase task scalability, so that segmentation can be performed in various types of surgeries with even novel surgical instruments. This project firstly works on building deep models with open world data in the surgical image processing field. 

Women carrying germline BRCA1/2 have a 65-70% lifetime risk of developing breast cancer. Current management options are either bilateral risk reducing mastectomy (RRM) or intensive screening, both of which can have a significant impact on those women who remain cancer free. Germline BRCA1/2 carriers would, therefore, benefit from additional tools to predict their risk of developing cancer. This is especially true in those patients carrying Variants of Uncertain Significance (VUS). We hypothesise that by implementing novel artificial intelligence (AI)-based computational approaches for digital pathology, we could identify subvisual morphometric phenotypes in the normal breast tissue, which are indicative of early signs of breast cancer. Our aim is to (i) advance our understanding of physiological processes underlying tumour initiation in seemingly histologically normal breast tissue; and (ii) to identify phenotypic characteristics of pre-malignant changes in this breast tissue to aid in tool development for early cancer detection in germline BRCA1/2 carriers.  We will study breast tissue from healthy women and prophylactic contralateral breast cancers. Digitised WSI of H&E and multiplex immunofluorescence-stained tissue will be explored by single cell resolution and spatial cellular configuration models. To handle the multi-dimensional data, the combination of signal processing and deep neural network methods is proposed, which can correlate complex morphological features whilst requiring less data. Identifying premalignant changes in the histologically normal breast of germline BRCA1/2 carriers will provide new avenues for early cancer detection. Healthy BRCA1/2 carriers could opt for periodical breast biopsy, avoiding unnecessary RRM. If validated prospectively, our tool provides a management option and could be included in the NHS clinical screening programme for germline BRCA1/2 carriers, with the intent to intervene if premalignant features are identified. Moreover, by identifying biomarkers for patient selection to ongoing therapeutic strategies, our tool could increase the efficacy of current breast cancer prevention agents and could be translatable to several early cancer detection approaches. 

My project is about developing deep-learning techniques for different application scenarios and aims to make the system more trustable and explainable. The first application scenario is about using deep learning methods to detect surgical instruments. And the second task is about metastases detection of breast cancer based on deep learning methods. For the first application scenario, deep learning can help to automatically detect the surgical instruments during surgery, assisting the surgeons to complete surgeries and automatically doing surgeries in the future. Therefore, a deep-learning-based system will be established. Now, I am paying attention to tip detection of catheter based on X-ray images. To detect the tiny point, an initial idea based on CNN is put forward, which can help to detect the endpoints in different resolutions based on more useful features learning from the images. For the second application scenario, deep-learning techniques can help doctors deal with plenty of Whole Slide Images (WSIs) to detect breast cancer, further reducing the misdiagnosis. Therefore, a system that can gain high detection accuracy will be proposed. And, to make the solution more trustable, a tool that can explain the system will be developed. I am mainly paying attention to develop a system to segment the metastases based on CNN and even consider how to make the whole system explainable. 

The project is part of a collaboration with OUCRU Vietnam and King’s College London, aiming the use of machine learning (ML) approaches to develop tools for diagnosis, staging and prognosis of Tuberculosis Brain Meningitis (TBM) patients. In detail, this project envisions the extraction of relevant features to characterise TBM and its different phenotypes during the course of the disease. Furthermore, we also aim to design a subject-specific disease progression model that can account for environmental and clinical factors, which might influence the patient outcome. 

U-Nets are trained to parcellate the human brain. 

Deep learning-based prostate cancer diagnosis tool. 

Creation of augmentation module in PyTorch compatible with Monai environment. Investigation of performance gains on the 3D segmentation using multiple different networks. 

The project aims to develop and validate deep learning models to predict, from MRI and clinical data, which tumours are likely to grow and require treatment. The student will be able to focus on designing novel radiomics analysis for vestibular schwannoma while exploiting an existing fully-automated AI tumour segmentation framework. This will enable clinicians to deliver personalised and standardised management plans to individual patients and has the potential to significantly reduce the number of required surveillance scans. 

This research project will investigate Visual Question Answering and Image Captioning. The first aims to find the correct answer to a given question that is consistent with the visual content of a given image while the second focuses on describing the content of an image with a natural language sentence. These technologies empower computers to better understand realistic visual information and transform it into natural language that we are familiar with, which can be used to build an intelligent human-computer interaction system. The project will involve the usage of public datasets, such as VQA-CP for Visual Question Answering, and MSCOCO for Image Captioning, as well as deep learning platforms, such as TensorFlow, PyTorch, etc. 

Adequate hydration is considered a significant factor in good physiological and physical health for human. Mild dehydration happens commonly among people and was proved to increase the risk of chronic diseases. It has been proved that dehydration is closely associated with disability, hospitalization and mortality. To avoid dehydration, many studies have stressed the significance of adequate drinking, which is the predominant way of body water supplying. However, low-intake dehydration, which refers to dehydration status caused by inadequate fluid intake, has been endangering public health and often underemphasized. Hence, fluid intake monitoring, which monitors the action and amount of drinking, is crucially needed for ensuring adequate drinking. 

Using machine learning to automatically detect standard planes and measure biometrics during the mid-pregnancy ultrasound scan. 

The objective of this research is to study deep learning methods for the visual recognition of urban and rural scenes in remote sensing imagery. Algorithms will be developed to detect and analyze the distribution of different objects such as buildings and trees in visual images; their applications in the geographical field will also be investigated. 

Multi-task learning is common in deep learning: For similar tasks like detection and segmentation, or detection and counting, this has already been achieved given the supervision of one for the other. There exists clear evidence that adding one side task would help the improvement of the main task, yet it is unclear how much benefits both tasks can get in these combinations, especially if they are not strongly correlated. For this reason, multiple tasks are normally processed independently in the current fashion. Another reason lies in the scalability of learning multiple tasks together in terms of both network optimization and practical implementation. To tackle this, careful designs of the conjunction of multiple tasks are needed; novel methodologies of learning paradigms are also expected. This project is placed in the endoscopic image processing domain. We aim to develop a machine learning model with general visual intelligence capacity in robotic surgery, which includes depth and optical flow estimation, surgical instrument detection and anatomy recognition, as well as surgical action recognition. Depth and optical flow estimation as well as anatomy recognition are key requirements to develop autonomous robotic control schemes that are cognizant of the surgical scene. Automatic detection and tracking of surgical instruments from laparoscopic surgery videos further plays an important role for providing advanced surgical assistance to the clinical team, given the uncertainties associated with surgical robots kinematic chains and the potential presence of tools not directly manipulated by the robot. Being able to know how many and where are the instruments finds its applications such as: placing informative overlays on the screen; performing augmented reality without occluding instruments; visual servoing; surgical task automation; etc.  Surgical action recognition is also critical to advance autonomous robotic assistance during the procedure and for automated auditing purposes. 

This project seeks to advance the state of the art in AI-based surgical tool detection and tracking by designing novel semi-supervised and weakly-supervised approaches able to achieve robust and real-time performance. Automatic detection and tracking of surgical tools from laparoscopic surgery videos is bound to play a key role in enabling surgery with autonomous features and for providing advanced surgical assistance to the clinical team. Being able to know how many and where the instruments are has a wealth of applications such as: placing informative overlays on the screen; performing augmented reality without occluding instruments; surgical workflow analysis; visual servoing; surgical task automation; etc. When the tools and vision system (endoscope) are robotically manipulated, one could expect the kinematic information of the robot to provide accurate tool positioning information. In practice, the large number of joints, cables and other sources of slack and hysteresis in the robots make the translation of kinematic information into calibrated positioning data mostly unrepeatable and prone to error. An alternative is to use the endoscope itself, which is already present, as a sensor. One of the first methods devised to help in the surgical tool detection process from videos was the placement of fiducial markers on the instruments. If the markers are remarkably different from observed tissue, the detection task would likely be solved through very simple processing. Yet, adding fiducials on surgical instruments has been strongly rejected by the medical device manufacturers as it presents diverse and important disadvantages (e.g. sterilisation, positioning, occlusion from blood, etc.). Training deep convolutional networks from manually annotated datasets of instrument detection and tracking in surgical scenes is a promising approach for this task. Yet, producing manual annotations of surgical videos is tedious, time-consuming and non-scalable given the evolutive nature of surgical technology and the expertise required for the annotation task. No large datasets comparable to industry standards in the computer vision community are available for these tasks. The lack of large-scale annotated datasets will remain a rate-limiting factor to achieve the robustness and accuracy required to use surgical detection and tracking in patient critical task such as autonomous surgery. As in the autonomous driving research field, automating the creation of synthetic realistic training datasets by exploiting advanced simulation may help improve the performance of deep learning approaches but a gap is likely to remain if only small amounts of real data are being exploited in addition to simulations. 

This research project will investigate few-shot learning in semantic segmentation. Few-shot learning is widely applied in many real problems where the training data are not sufficient for developing standard machine learning algorithms. While semantic segmentation which can be applied to geographic information systems, driverless technology, robots, and other fields aims to divide the different objects in the image at the pixel level and mark each pixel in the original image. The aim of this project is to develop few-shot learning models for reducing the cost of labor and generalizing on novel classes in semantic segmentation. The project will involve the usage of public datasets, such as COCO and PASCAL VOC, as well as deep learning platforms, such as TensorFlow, PyTorch, etc. Therefore, we need to apply for computing resources to process a large number of images and build deep-learning models. 

One of the directions of RL is goal-conditioned learning, which focuses on how an agent can effectively learn  the optimal policy by small supervision of the expert, some end goal which should be reached or a few subgoals in the process. It has a problem that these goals are usually very sparse and hard to reach the first time. Thus, the RL agent can only succeed with the exploration of heuristics. Partly, it can be solved by imitation learning when the goal is set for each frame/second. However, in real life, it is hard to supervise some robot as it has different dynamics compared to humans. Moreover, when humans learn new information or skill, they do not need the observations for each time point but instead supervised by some key steps.   In Dreamer paper, they have learnt the decoder from observations into compact latent space,  which was used to predict the future states and understand the policy. Such latent representation is a powerful tool that can help address the problem above. The main idea is to train representation learning, which will give us the encoder to provide latent representation for current  and goal images. Afterwards, the reward can be expressed in terms of Euclidean distance. 

Making TIAGo robot help the elderly or physically disabled people with daily household activities. 

Video data will be collected in a café and will be analysed to build a human approaching behaviour recognition algorithm. 

Use reinforcement learning method to solve problem in robot control, multi-agent control under safety consideration, involved with graph representation learning for improve performance and learning efficiency, also use fuzzy theory for increasing the explainability of the model 

Using ML tools to distinguish brain lesions in Tuberculosis Meningitis patients 

